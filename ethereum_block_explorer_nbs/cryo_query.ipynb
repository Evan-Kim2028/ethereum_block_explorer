{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cryoQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp cryo_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | export\n",
    "import jupyter_black\n",
    "import os\n",
    "import polars as pl\n",
    "import re\n",
    "import cryo\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(200)\n",
    "pl.Config.set_fmt_float(\"full\")\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/evan/Documents/ethereum_block_explorer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/Documents/ethereum_block_explorer/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class cryoQuery:\n",
    "    \"\"\"\n",
    "    `cryoQuery` is used to query data from cryo\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data_path: str = \"data/raw\"\n",
    "    rpc: str = \"https://eth.merkle.io\"\n",
    "\n",
    "    def _create_data_filepaths(self):\n",
    "        \"\"\"\n",
    "        Creates folders for storing raw data from cryo.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.raw_data_path):\n",
    "            os.makedirs(self.raw_data_path)\n",
    "            print(\"Data folder created.\")\n",
    "        else:\n",
    "            print(\"Data folder already exists.\")\n",
    "\n",
    "    def query_blocks_txs(\n",
    "        self,\n",
    "        n_error_threshold: int = 1,\n",
    "        retry_threshold: int = 5,\n",
    "        block_range: list[str] = [\"latest\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetches block and transaction data. Attempts retries up to 'retry_threshold'. If errors exceed 'n_error_threshold'.\n",
    "\n",
    "        :param n_error_threshold: The number of allowed errors before retrying the query.\n",
    "        :param retry_threshold: The maximum number of retries for the query.\n",
    "        :param block_range: The range of blocks to query. Defaults to [\"latest\"].\n",
    "        \"\"\"\n",
    "        self._create_data_filepaths()\n",
    "        n_errored = n_error_threshold + 1\n",
    "        retry_count = 0\n",
    "\n",
    "        # make cryo query\n",
    "        while retry_count < retry_threshold:\n",
    "            print(f\"Retry count: {retry_count}\")\n",
    "            retry_count += 1\n",
    "            if n_error_threshold < n_errored:\n",
    "                output: dict[str] = cryo.freeze(\n",
    "                    \"blocks_and_transactions\",\n",
    "                    blocks=block_range,\n",
    "                    hex=True,\n",
    "                    rpc=self.rpc,\n",
    "                    no_verbose=False,  # this doesn't seem to have any effect\n",
    "                    output_dir=\"data/raw\",\n",
    "                    subdirs=[\"datatype\"],\n",
    "                    include_columns=[\"n_rlp_bytes\"],\n",
    "                    exclude_columns=[\"input\", \"value\"],\n",
    "                    # compression=[\"zstd\"],\n",
    "                    compression=[\"lz4\"],  # bug, can't use zstd in cryo 0.3.0\n",
    "                )\n",
    "\n",
    "                n_errored = output[\"n_errored\"]\n",
    "                print(f\"Number of errors: {n_errored}\")\n",
    "            if n_errored == 0:\n",
    "                print(f\"{n_errored} == 0. Done!\")\n",
    "                break\n",
    "\n",
    "\n",
    "# | export\n",
    "@dataclass\n",
    "class cryoTransform:\n",
    "    \"\"\"\n",
    "    cryoTransform extends the underlying transactions and blocks dataset with extra columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def extend_txs_blocks(\n",
    "        self,\n",
    "        txs: pl.LazyFrame,\n",
    "        blocks: pl.LazyFrame,\n",
    "        mempool: pl.LazyFrame | None = None,\n",
    "    ) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Combines transaction, block, and optionally flashbots mempool data into a single LazyFrame.\n",
    "\n",
    "        Uses pattern matching to handle the optional mempool data.\n",
    "        If mempool data is provided, it is joined with the transactions and blocks data.\n",
    "        If not, only transactions and blocks are joined.\n",
    "\n",
    "        Preprocessing:\n",
    "        - add block percentile and ticks\n",
    "        - convert gas to gwei\n",
    "        - convert bytes to kilobytes\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        - txs (pl.LazyFrame): LazyFrame containing transaction data.\n",
    "        - blocks (pl.LazyFrame): LazyFrame containing block data.\n",
    "        - mempool (pl.LazyFrame, optional): LazyFrame containing mempool data. Default is None.\n",
    "\n",
    "        Returns:\n",
    "        - pl.LazyFrame: A unified LazyFrame with enriched transaction data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use pattern matching to handle the presence or absence of mempool data\n",
    "        match mempool:\n",
    "            case _ if isinstance(mempool, pl.LazyFrame):\n",
    "                # Join transactions with blocks and mempool data if mempool is provided\n",
    "                combined_df = txs.join(\n",
    "                    blocks, on=\"block_number\", how=\"left\", suffix=\"_block\"\n",
    "                ).join(\n",
    "                    mempool,\n",
    "                    right_on=\"hash\",\n",
    "                    left_on=\"transaction_hash\",\n",
    "                    how=\"left\",\n",
    "                    suffix=\"_mempool\",\n",
    "                )\n",
    "\n",
    "            case None:\n",
    "                # Join only transactions with blocks if mempool is not provided\n",
    "                combined_df = txs.join(\n",
    "                    blocks, on=\"block_number\", how=\"left\", suffix=\"_block\"\n",
    "                )\n",
    "\n",
    "        agg_df: pl.LazyFrame = combined_df.group_by(\"block_number\").agg(\n",
    "            [\n",
    "                pl.col(\"transaction_index\").max().alias(\"transaction_index_max\"),\n",
    "                pl.col(\"n_rlp_bytes\").sum().alias(\"block_encoded_bytes\"),\n",
    "                pl.col(\"n_input_bytes\").sum().alias(\"block_calldata_bytes\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            combined_df.join(agg_df, on=\"block_number\", how=\"left\")\n",
    "            .with_columns(\n",
    "                [\n",
    "                    # Calculate the transaction gas cost\n",
    "                    (pl.col(\"gas_used\") * pl.col(\"gas_price\") / 10**18).alias(\n",
    "                        \"tx_gas_cost\"\n",
    "                    ),\n",
    "                    # Convert epoch timestamp to datetime\n",
    "                    pl.from_epoch(\"timestamp\").alias(\"block_datetime\"),\n",
    "                    # Calculate the gas price premium over the base fee per gas\n",
    "                    (pl.col(\"gas_price\") / pl.col(\"base_fee_per_gas\")).alias(\n",
    "                        \"block_gas_premium\"\n",
    "                    ),\n",
    "                    (pl.col(\"base_fee_per_gas\").rolling_mean(window_size=7200)).alias(\n",
    "                        \"avg_base_fee_daily\"\n",
    "                    ),\n",
    "                                        (pl.col(\"base_fee_per_gas\").rolling_mean(window_size=300)).alias(\n",
    "                        \"avg_base_fee_hourly\"\n",
    "                    ),\n",
    "                    (pl.col(\"base_fee_per_gas\").rolling_mean(window_size=5)).alias(\n",
    "                        \"avg_base_fee_minute\"\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            .with_columns(\n",
    "                # Calculate the transaction index percentile within its block\n",
    "                (\n",
    "                    pl.col(\"transaction_index\") / pl.col(\"transaction_index_max\") * 100\n",
    "                ).alias(\"blockspace_percentile\")\n",
    "            )\n",
    "            .with_columns(\n",
    "                # Round the block space percentile for easier interpretation\n",
    "                (pl.col(\"blockspace_percentile\").round()).alias(\n",
    "                    \"rounded_blockspace_percentile\"\n",
    "                )\n",
    "            )\n",
    "            # unit conversions\n",
    "            .with_columns(\n",
    "                # convert gas to gwei\n",
    "                (pl.col(\"gas_price\") / 10**9).alias(\"gas_price_gwei\"),\n",
    "                (pl.col(\"max_priority_fee_per_gas\") / 10**9).alias(\n",
    "                    \"max_priority_fee_per_gas_gwei\"\n",
    "                ),\n",
    "                (pl.col(\"max_fee_per_gas\") / 10**9).alias(\"max_fee_per_gas_gwei\"),\n",
    "                (pl.col(\"base_fee_per_gas\") / 10**9).alias(\"base_fee_per_gas_gwei\"),\n",
    "                # convert bytes to kilobytes\n",
    "                (pl.col(\"block_encoded_bytes\") / 10**3).alias(\"block_encoded_kbytes\"),\n",
    "                (pl.col(\"block_calldata_bytes\") / 10**3).alias(\n",
    "                    \"block_calldata_kbytes\"\n",
    "                ),\n",
    "            )\n",
    "            .drop(\n",
    "                \"gas_price\",\n",
    "                \"max_priority_fee_per_gas\",\n",
    "                \"max_fee_per_gas\",\n",
    "                \"base_fee_per_gas\",\n",
    "                \"block_encoded_bytes\",\n",
    "                \"block_calldata_bytes\",\n",
    "            )\n",
    "            .fill_nan(0)  # Fill NaN values with 0\n",
    "            .unique()  # Ensure all rows are unique\n",
    "        )\n",
    "\n",
    "    # 1. read the files in the raw data from block number partitions, sync them together, perform a transformation, and then save into a new folder.\n",
    "    def read_filenames(self, directory) -> list:\n",
    "        try:\n",
    "            return sorted(\n",
    "                [\n",
    "                    f\n",
    "                    for f in os.listdir(directory)\n",
    "                    if os.path.isfile(os.path.join(directory, f))\n",
    "                ]\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "\n",
    "    def extract_block_batch_index(self, filename) -> list[tuple]:\n",
    "        match = re.search(r\"to_(\\d+)\", filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    def sync_filenames(self, directory_a: str, directory_b: str):\n",
    "        # Read filenames from both directories\n",
    "        transactions_filenames = self.read_filenames(directory_a)\n",
    "        blocks_filenames = self.read_filenames(directory_b)\n",
    "\n",
    "        # Extract numbers and create mappings\n",
    "        transactions_mapping = {\n",
    "            self.extract_block_batch_index(name): name\n",
    "            for name in transactions_filenames\n",
    "        }\n",
    "        blocks_mapping = {\n",
    "            self.extract_block_batch_index(name): name for name in blocks_filenames\n",
    "        }\n",
    "\n",
    "        # Find common keys\n",
    "        common_keys = set(transactions_mapping.keys()).intersection(\n",
    "            blocks_mapping.keys()\n",
    "        )\n",
    "\n",
    "        # Creating synced files dictionary\n",
    "        synced_files = {\n",
    "            directory_a: [transactions_mapping[key] for key in common_keys],\n",
    "            directory_b: [blocks_mapping[key] for key in common_keys],\n",
    "        }\n",
    "\n",
    "        return synced_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
